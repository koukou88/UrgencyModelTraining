{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khouloud\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = os.path.abspath('..\\Data')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.1\n",
    "VALIDATION_SPLIT = 0.1\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "label_dict = {}\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string) #remplacement\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "#string.strip(): Leading whitepsace are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(DIR_DATA + filename,delimiter=';')\n",
    "    selected = ['label', 'text']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    #labels=['negative', 'neutre', 'positive']\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "        #label_dict={'negative': 0, 'neutre': 1, 'positive': 2}\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    #to_categorical: Converts a class vector (integers) to binary class matrix\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmModel(embedding_matrix,epoch):\n",
    "    model = Sequential() # configure the model for training\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "    # n taille du vocabulaire du dataset et embedding_dims est la taille du vecteur de chaque mot(300) selon glove\n",
    "    \n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    # add layers\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # loss='categorical_crossentropy' for a multi-class classification problem\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = loadData('\\service5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "w =[nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khouloud\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(w, size=300, min_count = 1, window = 5)\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "pretrained_weights = model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data = createVocabAndData(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          586800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 806,706\n",
      "Trainable params: 219,906\n",
      "Non-trainable params: 586,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1099 samples, validate on 123 samples\n",
      "Epoch 1/40\n",
      "1099/1099 [==============================] - 5s 5ms/step - loss: 0.6182 - acc: 0.7006 - val_loss: 0.5408 - val_acc: 0.7724\n",
      "Epoch 2/40\n",
      "1099/1099 [==============================] - 3s 3ms/step - loss: 0.6047 - acc: 0.7179 - val_loss: 0.5592 - val_acc: 0.7724\n",
      "Epoch 3/40\n",
      "1099/1099 [==============================] - 4s 3ms/step - loss: 0.6016 - acc: 0.7179 - val_loss: 0.5388 - val_acc: 0.7724\n",
      "Epoch 4/40\n",
      "1099/1099 [==============================] - 4s 3ms/step - loss: 0.6005 - acc: 0.7179 - val_loss: 0.5470 - val_acc: 0.7724\n",
      "Epoch 5/40\n",
      "1099/1099 [==============================] - 4s 3ms/step - loss: 0.6012 - acc: 0.7179 - val_loss: 0.5556 - val_acc: 0.7724\n",
      "Epoch 6/40\n",
      "1099/1099 [==============================] - 4s 3ms/step - loss: 0.5950 - acc: 0.7179 - val_loss: 0.5403 - val_acc: 0.7724\n",
      "Epoch 7/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5987 - acc: 0.7179 - val_loss: 0.5423 - val_acc: 0.7724\n",
      "Epoch 8/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5983 - acc: 0.7179 - val_loss: 0.5509 - val_acc: 0.7724\n",
      "Epoch 9/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5969 - acc: 0.7179 - val_loss: 0.5451 - val_acc: 0.7724\n",
      "Epoch 10/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5993 - acc: 0.7179 - val_loss: 0.5396 - val_acc: 0.7724\n",
      "Epoch 11/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5972 - acc: 0.7179 - val_loss: 0.5505 - val_acc: 0.7724\n",
      "Epoch 12/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.6008 - acc: 0.7179 - val_loss: 0.5477 - val_acc: 0.7724\n",
      "Epoch 13/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5989 - acc: 0.7179 - val_loss: 0.5416 - val_acc: 0.7724\n",
      "Epoch 14/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5990 - acc: 0.7179 - val_loss: 0.5422 - val_acc: 0.7724\n",
      "Epoch 15/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5970 - acc: 0.7179 - val_loss: 0.5461 - val_acc: 0.7724\n",
      "Epoch 16/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5985 - acc: 0.7179 - val_loss: 0.5440 - val_acc: 0.7724\n",
      "Epoch 17/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.5982 - acc: 0.7179 - val_loss: 0.5430 - val_acc: 0.7724\n",
      "Epoch 18/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.6018 - acc: 0.7179 - val_loss: 0.5445 - val_acc: 0.7724\n",
      "Epoch 19/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5953 - acc: 0.7179 - val_loss: 0.5425 - val_acc: 0.7724\n",
      "Epoch 20/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.5968 - acc: 0.7179 - val_loss: 0.5443 - val_acc: 0.7724\n",
      "Epoch 21/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.5938 - acc: 0.7179 - val_loss: 0.5418 - val_acc: 0.7724\n",
      "Epoch 22/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5938 - acc: 0.7179 - val_loss: 0.5456 - val_acc: 0.7724\n",
      "Epoch 23/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5960 - acc: 0.7179 - val_loss: 0.5451 - val_acc: 0.7724\n",
      "Epoch 24/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5977 - acc: 0.7179 - val_loss: 0.5447 - val_acc: 0.7724\n",
      "Epoch 25/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5976 - acc: 0.7179 - val_loss: 0.5431 - val_acc: 0.7724\n",
      "Epoch 26/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5959 - acc: 0.7179 - val_loss: 0.5440 - val_acc: 0.7724\n",
      "Epoch 27/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5969 - acc: 0.7179 - val_loss: 0.5438 - val_acc: 0.7724\n",
      "Epoch 28/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5967 - acc: 0.7179 - val_loss: 0.5463 - val_acc: 0.7724\n",
      "Epoch 29/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5930 - acc: 0.7179 - val_loss: 0.5432 - val_acc: 0.7724\n",
      "Epoch 30/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5960 - acc: 0.7179 - val_loss: 0.5426 - val_acc: 0.7724\n",
      "Epoch 31/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.6005 - acc: 0.7179 - val_loss: 0.5452 - val_acc: 0.7724\n",
      "Epoch 32/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5966 - acc: 0.7179 - val_loss: 0.5451 - val_acc: 0.7724\n",
      "Epoch 33/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5983 - acc: 0.7179 - val_loss: 0.5419 - val_acc: 0.7724\n",
      "Epoch 34/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5953 - acc: 0.7179 - val_loss: 0.5436 - val_acc: 0.7724\n",
      "Epoch 35/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5972 - acc: 0.7179 - val_loss: 0.5441 - val_acc: 0.7724\n",
      "Epoch 36/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5938 - acc: 0.7179 - val_loss: 0.5428 - val_acc: 0.7724\n",
      "Epoch 37/40\n",
      "1099/1099 [==============================] - 5s 4ms/step - loss: 0.5962 - acc: 0.7179 - val_loss: 0.5423 - val_acc: 0.7724\n",
      "Epoch 38/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5971 - acc: 0.7179 - val_loss: 0.5453 - val_acc: 0.7724\n",
      "Epoch 39/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5938 - acc: 0.7179 - val_loss: 0.5427 - val_acc: 0.7724\n",
      "Epoch 40/40\n",
      "1099/1099 [==============================] - 4s 4ms/step - loss: 0.5962 - acc: 0.7179 - val_loss: 0.5450 - val_acc: 0.7724\n",
      "acc: 71.32%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "m=lstmModel(pretrained_weights,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

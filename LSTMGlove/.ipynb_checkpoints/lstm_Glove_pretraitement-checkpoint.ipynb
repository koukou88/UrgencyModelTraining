{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khouloud\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_GLOVE = os.path.abspath('..\\Glove')\n",
    "DIR_DATA = os.path.abspath('..\\Data')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.1\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloveVec(filename):\n",
    "    embeddings = {}\n",
    "    f = open(os.path.join(DIR_GLOVE, filename), encoding='utf-8')\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "    f.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(DIR_DATA + filename,delimiter=';')\n",
    "    selected = ['label', 'text']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    \n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(word_index,embeddings_index):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b1191f2f435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgloveVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove.840B.300d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-c5a37d03dbeb>\u001b[0m in \u001b[0;36mgloveVec\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDIR_GLOVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lstmModel(embedding_matrix,epoch):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    return model\n",
    "embeddings = gloveVec('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentences, labels = loadData('\\pretraitement_service5.csv')\n",
    "    \n",
    "    vocab, data = createVocabAndData(sentences)\n",
    "    embedding_mat = createEmbeddingMatrix(vocab,embeddings)\n",
    "    pickle.dump([data, labels, embedding_mat], open('embedding_matrix.pkl', 'wb'))\n",
    "    print (\"Data created\")\n",
    "\n",
    "    print(\"Train Test split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "    model=lstmModel(embedding_mat,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword={\"until\",\"off\",\"the\",\"any\",\"only\",\"d\",\"in\",\"because\",\"during\",\"on\",\"which\",\"who\",\"re\",\"then\",\"each\",\"will\",\"its\",\"these\",\"but\",\"other\",\"been\",\"t\",\"y\",\"between\",\"she's\",\"she\",\"have\",\"yourselves\",\"ve\",\"ain\",\"i\",\"just\",\"to\",\"that\",\"own\",\"before\",\"himself\",\"themselves\",\"a\",\"is\",\"my\",\"ll\",\"you\",\"where\",\"had\",\"too\",\"haven\",\"won\",\"yours\",\"over\",\"her\",\"his\",\"shan\",\"if\",\"after\",\"under\",\"such\",\"doing\",\"up\",\"through\",\"same\",\"we\",\"as\",\"or\",\"very\",\"no\",\"myself\",\"they\",\"me\",\"your\",\"are\",\"what\",\"against\",\"theirs\",\"ours\",\"and\",\"it's\",\"of\",\"did\",\"once\",\"am\",\"yourself\",\"again\",\"why\",\"above\",\"here\",\"into\",\"them\",\"at\",\"both\",\"s\",\"hers\",\"herself\",\"so\",\"by\",\"from\",\"when\",\"this\",\"that'll\",\"about\",\"him\",\"it\",\"was\",\"you've\",\"ma\",\"there\",\"ourselves\",\"their\",\"m\",\"how\",\"itself\",\"an\",\"with\",\"down\",\"more\",\"can\",\"some\",\"than\",\"our\",\"those\",\"do\",\"most\",\"be\",\"o\",\"were\",\"all\",\"out\",\"for\",\"has\",\"further\",\"while\",\"whom\",\"below\",\"he\",\"few\",\"being\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word(text):\n",
    "    switcher = {\n",
    "       \"couldn't\": \"could not\",\n",
    "       \"couldn\": \"could not\",\n",
    "       \"won't\": \"will not\",\n",
    "       \"won\": \"will not\",\n",
    "       \"mustn't\": \"must not\",\n",
    "       \"mustn\": \"must not\",\n",
    "       \"that'll\": \"that will\",\n",
    "       \"shouldn't\": \"should not\",\n",
    "       \"shouldn\": \"should not\",\n",
    "       \"should've\": \"should have\",\n",
    "       \"haven't\": \"have not\",\n",
    "       \"haven\": \"have not\",\n",
    "       \"hadn't\": \"have not\",\n",
    "       \"hadn\": \"have not\",\n",
    "       \"hasn't\": \"have not\",\n",
    "       \"hasn\": \"have not\",\n",
    "       \"didn't\": \"do not\",\n",
    "       \"didn\": \"do not\",\n",
    "       \"doesn't\": \"do not\",\n",
    "       \"doesn\": \"do not\",\n",
    "       \"don't\": \"do not\",\n",
    "       \"don\": \"do not\",\n",
    "       \"isn't\": \"be not\",\n",
    "       \"you'd\":\"you would\",\n",
    "       \"you've\":\"you have\",\n",
    "       \"you're\":\"you are\",\n",
    "       \"you'll\":\"you will\",\n",
    "       \"she's\":\"she is\",\n",
    "       \"it's\":\"it is\",\n",
    "       \"aren't\":\"are not\",\n",
    "       \"aren\":\"are not\",\n",
    "       \"weren't\":\"were not\",\n",
    "       \"weren\":\"were not\",\n",
    "       \"wouldn't\":\"would not\",\n",
    "       \"wouldn\":\"would not\",\n",
    "       \"needn't\":\"need not\",\n",
    "       \"needn\":\"need not\",\n",
    "       \"wasn't\":\"was not\",\n",
    "       \"wasn\":\"was not\",\n",
    "       \"mightn't\":\"might not\",\n",
    "       \"mightn\":\"might not\",\n",
    "       \"shan't\":\"shall not\",\n",
    "       \"shan\":\"shall not\",\n",
    "       \"can't\":\"can not\",\n",
    "       \"i'm\":\"i am\"\n",
    "   }\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    for i in range(len(text)):\n",
    "        text[i] = switcher.get(text[i], text[i])\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def transformText(text):\n",
    "    \n",
    "    text=split_alphanum(text)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    #replace_word\n",
    "    text=replace_word(text)\n",
    "    input_str=word_tokenize(text)\n",
    "    for i in range(len(input_str)):\n",
    "        input_str[i]=lemmatizer.lemmatize(input_str[i],pos='v')\n",
    "        input_str[i]=lemmatizer.lemmatize(input_str[i],pos='n')\n",
    "    \n",
    "    text=\" \".join(input_str)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "     #correcteur\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    for i in range(len(misspelled)):\n",
    "# Get the one `most likely` answer\n",
    "      word = spell.correction(misspelled[i])\n",
    "      misspelled[i]=word\n",
    "    text = \" \".join(misspelled)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in stopword]\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    \n",
    "    #Lematisation\n",
    "    #text = [WordNetLemmatizer().lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    \n",
    "    # Stemming\n",
    "    return gensim.parsing.preprocessing.stem_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/anurag3753/prediction-naive-bayes-preprocessing-with-gensim\n",
    "\n",
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import split_alphanum\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input_file,output_file):\n",
    "    df= pd.DataFrame(columns=['text', 'label'])\n",
    "    data = pd.read_csv(input_file,delimiter=';')\n",
    "    inputt=data.text\n",
    "    x_input = inputt.values\n",
    "    for i in x_input:\n",
    "        t=transformText(i)\n",
    "        t=np.array([i])\n",
    "        seq= tokenizer.texts_to_sequences(t)\n",
    "        seqs = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        yhat = model.predict(seqs)\n",
    "        class_pred = model.predict_classes(seqs)\n",
    "        classe=prediction_classe(class_pred)\n",
    "        df=df.append({'text':i,'label':classe},ignore_index=True)\n",
    "    df.to_csv(output_file,sep=';')\n",
    "    return data,df\n",
    "\n",
    "def prediction_classe(class_pred):\n",
    "    if class_pred[0]==0 :\n",
    "        classe='noturgent' \n",
    "    if class_pred[0]==1 :\n",
    "        classe='urgent'\n",
    "    return classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing # Help in preprocessing the data, very efficiently\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=\"You must now repair my phone!! \"\n",
    "t=transformText(t)\n",
    "t=np.array([t])\n",
    "text =[nltk.word_tokenize(sent) for sent in t]\n",
    "seq= tokenizer.texts_to_sequences(text)\n",
    "seqs = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "yhat = model.predict(seqs)\n",
    "print(yhat)\n",
    "class_pred = model.predict_classes(seqs)\n",
    "print(class_pred)\n",
    "print(prediction_classe(class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
